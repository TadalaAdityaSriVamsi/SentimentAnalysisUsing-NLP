{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1a9b58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data set done ........\n",
      "cool\n",
      "hello\n",
      "1999\n",
      "1999\n"
     ]
    }
   ],
   "source": [
    "\"\"\"new oos and preprocessing. \"\"\"\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import string\n",
    "import unicodedata\n",
    "from random import randint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier,BaggingClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import STOPWORDS, WordCloud\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB,CategoricalNB\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def transform(y_train):\n",
    "\n",
    "    y_train_vals = []\n",
    "    for i in y_train:\n",
    "        if i == 'love':\n",
    "            y_train_vals.append(0)\n",
    "        elif i == 'anger':\n",
    "            y_train_vals.append(1)\n",
    "        elif i == 'surprise':\n",
    "            y_train_vals.append(2)\n",
    "        elif i == 'joy':\n",
    "            y_train_vals.append(3)\n",
    "        elif i == 'sadness':\n",
    "            y_train_vals.append(4)\n",
    "        elif i == 'fear':\n",
    "            y_train_vals.append(5)\n",
    "    return y_train_vals\n",
    "\n",
    "\n",
    "def transform_decode(val):\n",
    "    y_vals = []\n",
    "    for i in val:\n",
    "        if i == 0:\n",
    "            y_vals.append(\"loveðŸ¥°\")\n",
    "        elif i == 1:\n",
    "            y_vals.append(\"angerðŸ˜¡\")\n",
    "        elif i == 2:\n",
    "            y_vals.append('surprise ðŸ˜®')\n",
    "        elif i == 3:\n",
    "            y_vals.append('joy ðŸ˜ƒ')\n",
    "        elif i ==4:\n",
    "            y_vals.append( 'sadness ðŸ˜”')\n",
    "        elif i == 5:\n",
    "            y_vals.append(\"fear ðŸ˜¨\")\n",
    "    return y_vals\n",
    "\n",
    "data_train = pd.read_csv(\"train.txt\",sep=\";\")\n",
    "# print(data_train)\n",
    "data_train = data_train.rename(columns={'i didnt feel humiliated':'text','sadness':'emotions'})\n",
    "# data_train.text = data_train['text']\n",
    "data_train.text = data_train.text.apply(str.lower)\n",
    "data_train.emotions = data_train.emotions.apply(str.lower)\n",
    "x_train = data_train['text']\n",
    "y_train = data_train['emotions']\n",
    "print(\"data set done ........\")\n",
    "data_test = pd.read_csv(\"test.txt\",sep=\";\")\n",
    "data_test = data_test.rename(columns={'im feeling rather rotten so im not very ambitious right now':'text','sadness':'emotions'})\n",
    "# print(data_test)\n",
    "data_test.text = data_test.text.apply(str.lower)\n",
    "data_test.emotions = data_test.emotions.apply(str.lower)\n",
    "\n",
    "x_test = data_test['text']\n",
    "y_test = data_test['emotions']\n",
    "\n",
    "from contractions import contractions_dict\n",
    "\n",
    "\n",
    "def expand_contractions(text, contraction_map=contractions_dict):\n",
    "    # Using regex for getting all contracted words\n",
    "    contractions_keys = '|'.join(contraction_map.keys())\n",
    "    contractions_pattern = re.compile(f'({contractions_keys})', flags=re.DOTALL)\n",
    "\n",
    "    def expand_match(contraction):\n",
    "        # Getting entire matched sub-string\n",
    "        match = contraction.group(0)\n",
    "        expanded_contraction = contraction_map.get(match)\n",
    "        if not expand_contractions:\n",
    "            print(match)\n",
    "            return match\n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "\n",
    "    return expanded_text\n",
    "\n",
    "\n",
    "# expand_contractions(\"i'd think y'all can do this.\")\n",
    "\n",
    "x_train.apply(expand_contractions)\n",
    "y_train.apply(expand_contractions)\n",
    "\n",
    "from cleantext import clean\n",
    "\n",
    "\n",
    "def rm_punc_from_word(word):\n",
    "    clean_alphabet_list = [\n",
    "        alphabet for alphabet in word if alphabet not in string.punctuation\n",
    "    ]\n",
    "    return ''.join(clean_alphabet_list)\n",
    "\n",
    "\n",
    "print(rm_punc_from_word('#cool!'))\n",
    "\n",
    "\n",
    "# Remove puncuation from text\n",
    "def rm_punc_from_text(text):\n",
    "    clean_word_list = [rm_punc_from_word(word) for word in text]\n",
    "    return ''.join(clean_word_list)\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "\n",
    "# nltk.download(\"stopwords\")\n",
    "\n",
    "def rm_number_from_text(text):\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return ' '.join(text.split())  # to rm `extra` white space\n",
    "\n",
    "def rm_stopwords_from_text(text):\n",
    "    _stopwords = stopwords.words('english')\n",
    "    text = text.split()\n",
    "    word_list = [word for word in text if word not in _stopwords]\n",
    "    return ' '.join(word_list)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = rm_punc_from_text(text)\n",
    "    text = rm_number_from_text(text)\n",
    "    text = rm_stopwords_from_text(text)\n",
    "\n",
    "    # there are hyphen(â€“) in many titles, so replacing it with empty str\n",
    "    # this hyphen(â€“) is different from normal hyphen(-)\n",
    "    text = re.sub('â€“', '', text)\n",
    "    text = ' '.join(text.split())  # removing `extra` white spaces\n",
    "\n",
    "    # Removing unnecessary characters from text\n",
    "    text = re.sub(\"(\\\\t)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"(\\\\r)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"(\\\\n)\", ' ', str(text)).lower()\n",
    "    # remove accented chars ('SÃ³mÄ› ÃccÄ›ntÄ›d tÄ›xt' => 'Some Accented text')\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode(\n",
    "        'utf-8', 'ignore'\n",
    "    )\n",
    "\n",
    "    text = re.sub(\"(__+)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"(--+)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"(~~+)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"(\\+\\++)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"(\\.\\.+)\", ' ', str(text)).lower()\n",
    "\n",
    "    text = re.sub(r\"[<>()|&Â©Ã¸\\[\\]\\'\\\",;?~*!]\", ' ', str(text)).lower()\n",
    "\n",
    "    text = re.sub(\"(mailto:)\", ' ', str(text)).lower()\n",
    "    text = re.sub(r\"(\\\\x9\\d)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"([iI][nN][cC]\\d+)\", 'INC_NUM', str(text)).lower()\n",
    "    text = re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", 'CM_NUM',\n",
    "                  str(text)).lower()\n",
    "    text = re.sub(\"(\\.\\s+)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"(\\-\\s+)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"(\\:\\s+)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"(\\s+.\\s+)\", ' ', str(text)).lower()\n",
    "\n",
    "    try:\n",
    "        url = re.search(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)', str(text))\n",
    "        repl_url = url.group(3)\n",
    "        text = re.sub(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)', repl_url, str(text))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    text = re.sub(\"(\\s+)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"(\\s+.\\s+)\", ' ', str(text)).lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "clean_text(\"Mrs. Robinson, 343 -- you're trying to fool &^%me, aren't you?\")\n",
    "x_train = x_train.apply(clean_text)\n",
    "x_test = x_test.apply(clean_text)\n",
    "y_train = y_train.apply(clean_text)\n",
    "y_test = y_test.apply(clean_text)\n",
    "Vectorizer = TfidfVectorizer()\n",
    "x_train = Vectorizer.fit_transform(x_train).toarray()\n",
    "y_train = transform(y_train)\n",
    "# print(y_train)\n",
    "print(\"hello\")\n",
    "x_test = Vectorizer.transform(x_test).toarray()\n",
    "print(len(x_test))\n",
    "y_test = transform(y_test)\n",
    "print(len(y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85e658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel =\"rbf\",random_state = 2)\n",
    "model.fit(x_train,y_train)\n",
    "print(\"model_trained\")\n",
    "y_pred = model.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabc3d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
